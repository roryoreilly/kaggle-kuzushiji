{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport tensorflow as tf\nimport os\nimport cv2\nimport regex as re\nimport math\nimport time\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D,Dropout, concatenate, UpSampling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_path = '/kaggle/input/kuzushiji-recognition'\nunicode_translation = os.path.join('/kaggle/input/kuzushiji-recognition','unicode_translation.csv')\ntrain_csv_path = os.path.join('/kaggle/input/kuzushiji-recognition','train.csv')\ntrain_image_path = os.path.join('/kaggle/input/kuzushiji-recognition','train_images')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(train_csv_path)\nprint(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unicodes = pd.read_csv(unicode_translation)\nprint(unicodes['Unicode'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_label_set(label_set):\n    label_item = split_label(label_set)\n    label_items = np.array(label_item).reshape(-1, 5)\n    return label_items\n\ndef split_label(label):\n    label = str(label)\n    return label.split(' ')\n\ndef image_path(id):\n    id = str(id)\n    if '.jpg' not in id:\n        return train_image_path + '/' + id + '.jpg'\n    \ndef show_image(image_id, label=None):\n    img = cv2.imread(os.path.join(image_path(image_id)))\n    if label:\n        unicode, x, y, w, h = splitLabel(label)\n        print(\"w: {}, h:{}\".format(w,h))\n        img = img[y:y+h, x:x+w]\n        plt.title(unicode)\n    plt.imshow(img)\n\ndef show_image_with_labels(df):\n    img = cv2.imread(os.path.join(image_path(df['image_id'])))\n    labels = split_label_set(df['labels'])\n    plt.imshow(img)\n    for label in labels:\n        unicode, x, y, w, h = split_label(label)\n        plt.gca().add_patch(patches.Rectangle((x,y),w,h,linewidth=1,edgecolor='r',facecolor='none'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"width = []\nfor i in range(500):\n    df = train_df.loc[i]\n    img = cv2.imread(os.path.join(image_path(df['image_id'])))\n    width = \n    print(np.shape(img))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mask(img, x, y, width, height):\n    #load the cropped area and apply an Otsu threshold\n    cropped_img = np.array(img[y:y+height,x:x+width,:])\n    blurred_img = cv2.GaussianBlur(cropped_img,(5,5),0)\n    img_gray = cv2.cvtColor(blurred_img, cv2.COLOR_BGR2GRAY)\n    ret, otsu = cv2.threshold(img_gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n    \n    #Place back the cropped area into a mask with the original image size\n    img_height, img_width = img.shape[:2]\n    img_mask = np.full((img_height,img_width),0)\n    img_mask[y:y+height,x:x+width] = otsu\n\n    return img_mask\n\ndef get_mask_for_df(df):\n    img = cv2.imread(os.path.join(image_path(df['image_id'])))\n    mask = np.zeros((img.shape[0], img.shape[1], 2), dtype='float32')\n    labels = df['labels']\n    if isinstance(labels, str):\n        labels = split_label_set(labels)\n\n        mask = np.full((img.shape[:2]),0)\n        for i, label in enumerate(labels):\n            x, y, w, h = (int(t) for t in label[1:])\n            mask = mask + get_mask(img, x, y, w, h)\n    return mask\n\n# for i in range(len(train_df.index)):\nfor i in range(1):\n    df = train_df.iloc[i]\n    mask = get_mask_for_df(df)\n    mask_path = \"{}.csv\".format(str(df['image_id']))\n    np.savetxt(mask_path, mask, delimiter=\",\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mask_new(df):\n    img = cv2.imread(os.path.join(image_path(df['image_id'])))\n    mask = np.zeros((img.shape[0], img.shape[1], 2), dtype='float32')\n    labels = df['labels']\n    if isinstance(labels, str):\n        labels = np.array(labels.split(' ')).reshape(-1, 5)\n        for char, x, y, w, h in labels:\n            x, y, w, h = int(x), int(y), int(w), int(h)\n            if x + w >= img.shape[1] or y + h >= img.shape[0]:\n                continue\n            mask[y: y + h, x: x + w, 0] = 1\n            radius = 6\n            mask[y + h // 2 - radius: y + h // 2 + radius + 1, x + w // 2 - radius: x + w // 2 + radius + 1, 1] = 1\n    return mask\n\ndef display_mask(mask, width=None, height=None):\n    box = mask[:, :, 0]\n    centers = mask[:, :, 1]\n    \n    if width:\n        box = cv2.resize(box, (width, height))\n        centers = cv2.resize(centers, (width, height))\n    fig, axs = plt.subplots(1, 2, figsize=(8, 8))\n    axs[0].imshow(box, interpolation='bilinear')\n    axs[1].imshow(centers, interpolation='bilinear')\n    plt.show()\n    \ndf = train_df.iloc[0]\n# show_image_with_labels(df)\nmask= get_mask_new(df)\ndisplay_mask(mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def data_gen(dataframe, batch_size, img_width=256, img_height=256):\n#     while (True):\n#         img = np.zeros((batch_size, img_width, img_height, 3)).astype('float')\n#         mask = np.zeros((batch_size, img_width, img_height, 1)).astype('float')\n#         df_sample = dataframe.sample(batch_size)\n#         for index, df in df_sample.iterrows():\n#             i = 0\n#             train_img = cv2.imread(os.path.join(image_path(df['image_id'])))\n            \n#             train_mask = get_mask_new(train_img, df['labels'])\n#             train_mask = (train_mask[:,:,0]) / 255.\n#             train_mask = cv2.resize(train_mask, (img_width, img_height))\n#             train_mask = train_mask.reshape(img_width, img_height, 1) # Add extra dimension for parity with train_img size [512 * 512 * 3]\n#             mask[i] = train_mask\n            \n#             train_img = train_img / 255.\n#             train_img =  cv2.resize(train_img, (img_width, img_height))# Read an image from folder and resize\n#             img[i] = train_img #add to array - img[0], img[1], and so on.\n#             i += 1\n\n#         yield img, mask\n        \n# train, validate = train_test_split(train_df, test_size=0.2)\n# train_generator = data_gen(train,16)\n# validate_generator = data_gen(validate,16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class KuzushijiDataset(Sequence):\n    def __init__(self, dataframe, batch_size, img_width=256, img_height=256, training=False):\n        self.dataframe = dataframe\n        self.batch_size = batch_size\n        self.img_width = img_width\n        self.img_height = img_height\n        self.training = training\n        self.index = 0\n\n    def __len__(self):\n        return math.ceil(len(df.index) / self.batch_size)\n\n    def __getitem__(self, index):\n        img = np.zeros((self.batch_size, self.img_width, self.img_height, 3)).astype('float')\n        mask = np.zeros((self.batch_size, self.img_width, self.img_height, 1)).astype('float')\n        if self.training:\n            df_sample = self.dataframe.sample(self.batch_size)\n        else:\n            df_sample = self.dataframe[self.index:self.index+self.batch_size]\n            \n        i = 0\n        for index, df in df_sample.iterrows():\n            train_mask, train_img = self.get_img_and_mask(df)\n            mask[i] = train_mask\n            img[i] = train_img\n            i = i + 1\n            \n        self.index = self.index + self.batch_size\n        if (self.index + self.batch_size) > len(self.dataframe.index):\n            self.index = 0\n\n        return img, mask\n    \n    def get_img_and_mask(self, df):\n        train_mask = get_mask_new(df) / 255.\n        train_mask = cv2.resize(train_mask[:,:,0], (self.img_width, self.img_height))\n        train_mask = train_mask.reshape(self.img_width, self.img_height, 1) # Add extra dimension for parity with train_img size [512 * 512 * 3]\n        train_img = cv2.imread(os.path.join(image_path(df['image_id']))) / 255.\n        train_img =  cv2.resize(train_img, (self.img_width, self.img_height))# Read an image from folder and resize\n        return train_mask, train_img\n    \ntrain, validate = train_test_split(train_df, test_size=0.1)\ntrain_generator = KuzushijiDataset(train,16, training=True)\nvalidate_generator = KuzushijiDataset(validate,16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class KuzushijiDataset(utils.Dataset):\n\n    def __init__(self, df):\n        super().__init__(self)\n        \n        # Add classes\n        for i, name in enumerate(kuzushiji_to_detect):\n            self.add_class(\"kuzushiji\", i+1, name)\n        \n        # Add images \n        for i, row in df.iterrows():\n            self.add_image(\"kuzushiji\", \n                           image_id=row.name, \n                           path='../../input/kuzushiji-recognition/train_images/'+str(row.image_id)+\".jpg\", \n                           labels=row['CategoryId'],\n                           annotations=row['EncodedPixels'], \n                           height=row['Height'], width=row['Width'])\n\n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path'], [kuzushiji_to_detect[int(x)] for x in info['labels']]\n        \n    def load_image(self, image_id):\n        return resize_image(self.image_info[image_id]['path'])\n        \n\n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n                \n        mask = np.zeros((IMAGE_SIZE, IMAGE_SIZE, len(info['annotations'])), dtype=np.uint8)\n        labels = []\n        for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):\n            sub_mask = np.full(info['height']*info['width'], 0, dtype=np.uint8)\n            annotation = [int(x) for x in annotation.split(' ')]\n            \n            for i, start_pixel in enumerate(annotation[::2]):\n                sub_mask[start_pixel: start_pixel+annotation[2*i+1]] = 1\n\n            sub_mask = sub_mask.reshape((info['height'], info['width']), order='F')\n            sub_mask = cv2.resize(sub_mask, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n            \n            mask[:, :, m] = sub_mask\n            labels.append(int(label)+1)\n            \n        return mask, np.array(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for img, mask in train_generator:\n    for i in range(4):\n        fig, axs = plt.subplots(1, 2, figsize=(8, 8))\n        axs[0].imshow(img[i])\n        axs[1].imshow(mask[i,:,:,0])\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(y_true, y_pred):\n    def dice_loss(y_true, y_pred):\n        numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=(1,2,3))\n        denominator = tf.reduce_sum(y_true + y_pred, axis=(1,2,3))\n\n        return tf.reshape(1 - numerator / denominator, (-1, 1, 1))\n\n    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unet(pretrained_weights = None,input_size = (256,256,3)):\n    input_layer = Input(shape=input_size)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_layer)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n    merge6 = concatenate([drop4,up6], axis = 3)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n    merge9 = concatenate([conv1,up9], axis = 3)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n\n    model = tf.keras.Model(inputs=input_layer, outputs = conv10)\n\n    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy')\n    \n    model.summary()\n\n    if(pretrained_weights):\n    \tmodel.load_weights(pretrained_weights)\n\n    return model\n\nmodel = unet()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel.reset_states()\nsteps = 1\nepochs = 250\ncheckpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True)\nhistory = model.fit_generator(train_generator, \n                              steps_per_epoch=steps, \n                              epochs=epochs,\n                              validation_data=validate_generator,\n                              validation_steps=steps,\n                              callbacks=[checkpointer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validate'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_prediction(image_path, img_width=256, img_height=256):\n    img = cv2.imread(os.path.join(image_path))\n    height, width, channels = img.shape\n    test_img = img / 255.\n    test_img =  cv2.resize(test_img, (img_width, img_height))# Read an image from folder and resize\n    test_img = np.expand_dims(test_img, axis=0)\n    prediction_mask = model.predict(test_img) * 255.\n#     display_mask(prediction_mask[0], width, height)\n    prediction_mask = cv2.resize(prediction_mask[0], (width, height))\n    print(prediction_mask)\n    plt.imshow(prediction_mask, cmap='hot')\n    plt.show()\n    \n    \n    \ndf = validate.iloc[1]\nshow_prediction(image_path(df['image_id']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for img, mask in validate_generator:\n    break\n    \n\nmasks = model.predict(img) * 255.\nfor i in range(8):\n    fig, axs = plt.subplots(1, 2, figsize=(8, 8))\n    mask = cv2.resize(masks[i], (256, 256))\n    axs[0].imshow(img[i])\n    axs[1].imshow(mask)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}